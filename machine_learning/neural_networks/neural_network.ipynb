{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load AIS data\n",
    "data = pd.read_csv('../../data/RotHam_cleaned/rotterdam_hamburg_clean_new.csv')\n",
    "\n",
    "# Extract features and target variable in minutes\n",
    "features = data[['Latitude', 'Longitude', 'SOG', 'COG', 'TH', 'shiptype', 'pastTravelTime', 'TripID', 'MMSI']]\n",
    "target = data['timeTillArrival']/60\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate neural networks with gradient descent\n",
    "Here we have test model for a neural network using adam as a gradient descent optimization.\n",
    "As the loss function we use the mean squared error and our metric just as for the other models we use the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 16:21:20.453064: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-13 16:21:20.535461: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-13 16:21:21.189375: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-13 16:21:21.197334: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 16:21:23.128236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8648/8648 [==============================] - 22s 2ms/step - loss: 11384.2139 - mae: 56.3112 - val_loss: 3051.8596 - val_mae: 40.3707\n",
      "Epoch 2/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 3041.7603 - mae: 40.0611 - val_loss: 2954.7478 - val_mae: 39.4195\n",
      "Epoch 3/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2944.9875 - mae: 39.3180 - val_loss: 2826.7761 - val_mae: 38.3587\n",
      "Epoch 4/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2900.6008 - mae: 38.9348 - val_loss: 2794.3811 - val_mae: 38.3024\n",
      "Epoch 5/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2866.8433 - mae: 38.6625 - val_loss: 2770.0400 - val_mae: 38.0211\n",
      "Epoch 6/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2843.2380 - mae: 38.4715 - val_loss: 2827.8176 - val_mae: 38.2406\n",
      "Epoch 7/50\n",
      "8648/8648 [==============================] - 22s 3ms/step - loss: 2815.9324 - mae: 38.2550 - val_loss: 2754.0522 - val_mae: 37.8961\n",
      "Epoch 8/50\n",
      "8648/8648 [==============================] - 24s 3ms/step - loss: 2794.0647 - mae: 38.0472 - val_loss: 2836.9316 - val_mae: 38.2136\n",
      "Epoch 9/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2780.7085 - mae: 37.9073 - val_loss: 2709.6699 - val_mae: 37.7210\n",
      "Epoch 10/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2769.1487 - mae: 37.8332 - val_loss: 2703.5376 - val_mae: 37.2867\n",
      "Epoch 11/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2752.5334 - mae: 37.6868 - val_loss: 2888.6499 - val_mae: 38.4343\n",
      "Epoch 12/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2735.3494 - mae: 37.5546 - val_loss: 2676.0569 - val_mae: 37.1151\n",
      "Epoch 13/50\n",
      "8648/8648 [==============================] - 19s 2ms/step - loss: 2728.9568 - mae: 37.4651 - val_loss: 2780.1238 - val_mae: 37.8485\n",
      "Epoch 14/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2715.6765 - mae: 37.3397 - val_loss: 2627.5173 - val_mae: 36.6000\n",
      "Epoch 15/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2707.1162 - mae: 37.2670 - val_loss: 2656.4478 - val_mae: 37.0093\n",
      "Epoch 16/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2695.7524 - mae: 37.1246 - val_loss: 2682.8181 - val_mae: 37.2812\n",
      "Epoch 17/50\n",
      "8648/8648 [==============================] - 22s 3ms/step - loss: 2678.1941 - mae: 36.9488 - val_loss: 2852.3416 - val_mae: 38.3580\n",
      "Epoch 18/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2673.4194 - mae: 36.8639 - val_loss: 2615.0293 - val_mae: 36.4541\n",
      "Epoch 19/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2662.7578 - mae: 36.7762 - val_loss: 2914.6494 - val_mae: 38.0712\n",
      "Epoch 20/50\n",
      "8648/8648 [==============================] - 22s 3ms/step - loss: 2652.3779 - mae: 36.6434 - val_loss: 2589.5747 - val_mae: 36.2065\n",
      "Epoch 21/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2639.3240 - mae: 36.5425 - val_loss: 2711.0774 - val_mae: 36.9560\n",
      "Epoch 22/50\n",
      "8648/8648 [==============================] - 25s 3ms/step - loss: 2635.9475 - mae: 36.4937 - val_loss: 2638.0750 - val_mae: 36.5132\n",
      "Epoch 23/50\n",
      "8648/8648 [==============================] - 23s 3ms/step - loss: 2621.0959 - mae: 36.3652 - val_loss: 2543.3347 - val_mae: 35.9455\n",
      "Epoch 24/50\n",
      "8648/8648 [==============================] - 24s 3ms/step - loss: 2612.2156 - mae: 36.2717 - val_loss: 2539.8372 - val_mae: 35.8556\n",
      "Epoch 25/50\n",
      "8648/8648 [==============================] - 18s 2ms/step - loss: 2599.9463 - mae: 36.1337 - val_loss: 2569.5098 - val_mae: 35.9028\n",
      "Epoch 26/50\n",
      "8648/8648 [==============================] - 19s 2ms/step - loss: 2580.0554 - mae: 35.9852 - val_loss: 2512.5110 - val_mae: 35.6025\n",
      "Epoch 27/50\n",
      "8648/8648 [==============================] - 18s 2ms/step - loss: 2566.4016 - mae: 35.8962 - val_loss: 2549.5078 - val_mae: 35.9159\n",
      "Epoch 28/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2553.5969 - mae: 35.8024 - val_loss: 2583.1428 - val_mae: 35.8273\n",
      "Epoch 29/50\n",
      "8648/8648 [==============================] - 22s 3ms/step - loss: 2534.2844 - mae: 35.6530 - val_loss: 2559.8884 - val_mae: 35.7557\n",
      "Epoch 30/50\n",
      "8648/8648 [==============================] - 18s 2ms/step - loss: 2522.7888 - mae: 35.6020 - val_loss: 2458.2913 - val_mae: 35.2504\n",
      "Epoch 31/50\n",
      "8648/8648 [==============================] - 23s 3ms/step - loss: 2511.6604 - mae: 35.5488 - val_loss: 2445.6694 - val_mae: 35.2444\n",
      "Epoch 32/50\n",
      "8648/8648 [==============================] - 19s 2ms/step - loss: 2504.7371 - mae: 35.5206 - val_loss: 2479.9492 - val_mae: 35.6395\n",
      "Epoch 33/50\n",
      "8648/8648 [==============================] - 24s 3ms/step - loss: 2484.3386 - mae: 35.4024 - val_loss: 2421.8169 - val_mae: 34.9432\n",
      "Epoch 34/50\n",
      "8648/8648 [==============================] - 23s 3ms/step - loss: 2476.8916 - mae: 35.3313 - val_loss: 2497.8650 - val_mae: 35.7211\n",
      "Epoch 35/50\n",
      "8648/8648 [==============================] - 17s 2ms/step - loss: 2464.7188 - mae: 35.2609 - val_loss: 2391.8821 - val_mae: 34.7537\n",
      "Epoch 36/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2440.7825 - mae: 35.0844 - val_loss: 2413.3704 - val_mae: 34.7292\n",
      "Epoch 37/50\n",
      "8648/8648 [==============================] - 19s 2ms/step - loss: 2424.5039 - mae: 34.9668 - val_loss: 2466.3560 - val_mae: 35.9249\n",
      "Epoch 38/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2406.3875 - mae: 34.8274 - val_loss: 2529.9050 - val_mae: 35.7488\n",
      "Epoch 39/50\n",
      "8648/8648 [==============================] - 17s 2ms/step - loss: 2390.3162 - mae: 34.7153 - val_loss: 2339.3176 - val_mae: 34.3667\n",
      "Epoch 40/50\n",
      "8648/8648 [==============================] - 22s 3ms/step - loss: 2364.4175 - mae: 34.5055 - val_loss: 2324.3259 - val_mae: 34.2647\n",
      "Epoch 41/50\n",
      "8648/8648 [==============================] - 19s 2ms/step - loss: 2355.8262 - mae: 34.4535 - val_loss: 2340.5513 - val_mae: 34.2359\n",
      "Epoch 42/50\n",
      "8648/8648 [==============================] - 15s 2ms/step - loss: 2344.8809 - mae: 34.3644 - val_loss: 2282.7654 - val_mae: 33.8511\n",
      "Epoch 43/50\n",
      "8648/8648 [==============================] - 16s 2ms/step - loss: 2325.7354 - mae: 34.1863 - val_loss: 2542.1731 - val_mae: 36.2917\n",
      "Epoch 44/50\n",
      "8648/8648 [==============================] - 17s 2ms/step - loss: 2317.5662 - mae: 34.1422 - val_loss: 2257.8289 - val_mae: 33.7907\n",
      "Epoch 45/50\n",
      "8648/8648 [==============================] - 17s 2ms/step - loss: 2298.8943 - mae: 33.9917 - val_loss: 2291.2280 - val_mae: 33.6710\n",
      "Epoch 46/50\n",
      "8648/8648 [==============================] - 16s 2ms/step - loss: 2286.3538 - mae: 33.8626 - val_loss: 2275.9978 - val_mae: 33.6687\n",
      "Epoch 47/50\n",
      "8648/8648 [==============================] - 16s 2ms/step - loss: 2269.2917 - mae: 33.7080 - val_loss: 2242.4871 - val_mae: 33.4480\n",
      "Epoch 48/50\n",
      "8648/8648 [==============================] - 16s 2ms/step - loss: 2259.1155 - mae: 33.6072 - val_loss: 2235.7778 - val_mae: 33.5886\n",
      "Epoch 49/50\n",
      "8648/8648 [==============================] - 20s 2ms/step - loss: 2244.3484 - mae: 33.4852 - val_loss: 2226.1968 - val_mae: 33.2833\n",
      "Epoch 50/50\n",
      "8648/8648 [==============================] - 21s 2ms/step - loss: 2223.9399 - mae: 33.2897 - val_loss: 2263.5366 - val_mae: 34.1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1c8877eaf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer= 'adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2703/2703 [==============================] - 4s 1ms/step - loss: 2266.7029 - mae: 34.0756\n",
      "Test MAE: 34.075599670410156\n",
      "2703/2703 [==============================] - 3s 1ms/step\n",
      "Test MSE: 2266.7044523213426\n",
      "Test RMSE: 47.60991968404633\n",
      "R-squared: 0.9842645015218698\n",
      "Test MSA: 34.075582352507936\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'Test MAE: {mae}')\n",
    "\n",
    "# Predict travel times for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics if needed\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'Test MSE: {mse}')\n",
    "print(f'Test RMSE: {rmse}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Test MSA: {mae}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
